---
title: "Model Results"
author: "Evan Schwartz, Ethan Wood, Max Van Fleet"
date: "2024-11-14"
output:
  pdf_document: default
  html_document: default
editor_options:
  markdown:
    wrap: 72
---

# Loading data and Libraries

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(readr)
library(caret)
library(leaps)
library(glmnet)
library(dplyr)
library(MASS)
library(ggplot2)
library(lattice)

data = read_csv("final_data_tranform_clean.csv")

#Last Minute data cleaning, some economic vairables removed due to high collinearity 
data = dplyr::select(data, -c(1,4,6, 8,9, 10, 12, 13))
```

# K-folds for data=Here we prepare the training data by shuffling and preforming a 90%-10% training-test split.

```{r}
# 90/10 data
set.seed(664)

shuffleddata = data[sample(nrow(data)), ]
data_naive_shuff <- dplyr::select(shuffleddata, c(1, 2, 3, 6, 7,10, 28:59, 70:77,))


# Calculate the index for the 90-10 split
index90 = floor(0.9 * nrow(shuffleddata))

train = shuffleddata[1:index90,]
train_naive = data_naive_shuff[1:index90,]
# Create the 10% testing data
test = shuffleddata[(index90 + 1):nrow(shuffleddata),]
test_naive = data_naive_shuff[(index90 + 1):nrow(data_naive_shuff),]

#of the 90 data, 7 fold cross validation

folds = createFolds(train$loglatestPrice, k = 7, list = TRUE)
```

# AIC and BIC

```{r}
MSE.bic = vector()
bic.num = vector()
MSE.aic = vector()
aic.num = vector()

for (j in 1:7) {
  #setting respective fold to either training or testing
  train_data = train[-folds[[j]], ] #eliminate columns in testing data
  test_data = train[folds[[j]], ]
  
  #doing best subset selection on our data
  regfit.train = regsubsets(loglatestPrice ~ ., data = train_data,  method= "backward" )
  reg.summary = summary(regfit.train)
  
  #selecting which subset is the best, specifically selecting how many variables it has 
  bic.number = which.min(reg.summary$bic)
  bic.num[j] = bic.number

  #creating the model with the respective number of coefficents
  bic.mdl = coef(regfit.train, bic.number)
  
  # testing the models that are created
  test.mat2 = model.matrix(loglatestPrice ~ ., data = test_data)
  bic.pred = test.mat2[, names(bic.mdl)] %*% bic.mdl
  
  
  # Start with the full model
  full.model = lm(loglatestPrice ~ ., data = train_data)
  
  # Perform backward stepwise selection using AIC
  #stepwise.model = stepAIC(full.model, direction = "backward", trace = FALSE)

  #predict with the 
  #aic.pred <- predict(stepwise.model, newdata = test_data)
  #aic.num[j] <- length(coef(stepwise.model))


  #computing the MSE for each model
  MSE.bic[j] = mean(( test_data$loglatestPrice - bic.pred)**2)
  #MSE.aic[j] = mean((test_data$loglatestPrice - aic.pred)**2)
}

paste("Estimated MSE for BIC is ", sum(MSE.bic))
#paste("Estimated MSE for AIC is ", sum(MSE.aic))
 
bic.num
```

BIC says model with \_\_\_ variables, AIC says model with \_\_\_
variables

# LASSO & Ridge

## LASSO

```{r}
set.seed(1234)

vectorMSElasso = vector()
for (j in 1:7) {
  
  
  train_data = train[-folds[[j]], ]
  test_data = train[folds[[j]], ]
  
  train_data_x = as.matrix(train_data[,-6])
  train_data_y = as.matrix(train_data[, 6])
  test_data_x = as.matrix(test_data[,-6])
  test_data_y = as.matrix(test_data[, 6])
  
  grid = 10^seq(10, -2, length = 100)
  
  lasso.mod.cv = cv.glmnet(train_data_x, train_data_y, alpha = 1, lambda = grid, nfolds = 5)
  
  bestlambda = lasso.mod.cv$lambda.min
  
  lasso.fulldata = glmnet(train_data_x, train_data_y, alpha = 1, lambda = grid)
  lasso.coef = predict(lasso.fulldata , type = "coefficients",s = bestlambda)
  
  lasso.predict = predict(lasso.fulldata, s = bestlambda, newx = test_data_x)
  mse.lasso = mean((test_data_y - lasso.predict)^2)
  vectorMSElasso[j] = mse.lasso
}

mean(vectorMSElasso)
```

## Ridge

```{r}
set.seed(1234)

vectorMSEridge = vector()
for (j in 1:7) {
  
  
  train_data = train[-folds[[j]], ]
  test_data = train[folds[[j]], ]
  
  train_data_x = as.matrix(train_data[,-6])
  train_data_y = as.matrix(train_data[, 6])
  test_data_x = as.matrix(test_data[,-6])
  test_data_y = as.matrix(test_data[, 6])
  
  grid = 10^seq(10, -2, length = 100)
  
  lasso.mod.cv = cv.glmnet(train_data_x, train_data_y, alpha = 0, lambda = grid, nfolds = 5)
  
  bestlambda = lasso.mod.cv$lambda.min
  
  lasso.fulldata = glmnet(train_data_x, train_data_y, alpha = 0, lambda = grid)
  lasso.coef = predict(lasso.fulldata , type = "coefficients",s = bestlambda)
  
  lasso.predict = predict(lasso.fulldata, s = bestlambda, newx = test_data_x)
  mse.lasso = mean((test_data_y - lasso.predict)^2)
  vectorMSEridge[j] = mse.lasso
}
mean(vectorMSEridge)
```


# The Optimal model

```{r}
#lowest mean squared error was Ridge, so we will run this over entire training dataset
train_data_x = as.matrix(train[,-6])
train_data_y = as.matrix(train[, 6])
test_data_x = as.matrix(test[,-6])
test_data_y = as.matrix(test[, 6])
  
grid = 10^seq(10, -2, length = 100)
  
ridge.mod.cv = cv.glmnet(train_data_x, train_data_y, alpha = 0, lambda = grid, nfolds = 5)
plot(ridge.mod.cv, main = "MSE of Ridge with Respective Lambdas")

bestlambda = ridge.mod.cv$lambda.min


lasso.fulldata = glmnet(train_data_x, train_data_y, alpha = 0, lambda = grid)
ridge.coef = predict(lasso.fulldata , type = "coefficients",s = bestlambda)


ridge.predict = predict(lasso.fulldata, s = bestlambda, newx = test_data_x)
mse.ridge = mean((test_data_y - ridge.predict)^2)
paste("mean squared error on test data for ridge is ", mse.ridge)


paste("best lambda for ridge is ", bestlambda)

```






# Comparison with Naive Model

As noted in our report we now consider a model based on what some
relevant field experts consider to be the main features most buyers use
when valuing houses.

```{r}
# Fit the normal linear regression model for naive data 
lm_model_naive = lm(train_naive$loglatestPrice ~ ., data = train_naive)

naive_predictions <- predict(lm_model_naive, newdata = test_naive)

mse_naive <- mean((test_naive$loglatestPrice - naive_predictions)^2)

paste('Our coefficients form a linear model with MSE', mse_values[opt_threshold_num], 'and the naive model produces MSE',mse_naive,'. We may thus conclude our the naive model may actually be superior.')
```

## Residual Plots

```{r}
# Residual plot for the naive model
par(mfrow = c(1, 1))  # Set up two plots side by side

plot(naive_predictions, test_naive$loglatestPrice - naive_predictions, 
     main = "Residuals for Naive Model", 
     xlab = "Predicted Values", 
     ylab = "Residuals")
abline(h = 0, col = "red")


# Plot residuals for the optimal model
plot(ridge.predict, test_data_y - ridge.predict, 
     main = "Residuals for Optimal Model", 
     xlab = "Predicted Values", 
     ylab = "Residuals")
abline(h = 0, col = "red")

# Reset the plot layout
#par(mfrow = c(1, 1))
```

