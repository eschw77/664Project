---
title: "Model Results"
author: "Evan Schwartz, Ethan Wood, Max Van Fleet"
date: "2024-11-14"
output:
  pdf_document: default
  html_document: default
editor_options:
  markdown:
    wrap: 72
---

# Loading data and Libraries

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(readr)
library(caret)
library(leaps)
library(glmnet)
library(dplyr)
library(MASS)
library(ggplot2)
library(lattice)
library(lmridge)

data = read_csv("/Users/evanschwartz/664Project/Data/final_data_tranform_clean.csv", show_col_types = FALSE)

#Last Minute data cleaning, some economic vairables removed due to high collinearity 
data = dplyr::select(data, -c(1,4,6, 8,9, 10, 12, 13))
```

# K-folds for data=Here we prepare the training data by shuffling and preforming a 90%-10% training-test split.

```{r}
# 90/10 data
set.seed(664)

shuffleddata = data[sample(nrow(data)), ]
data_naive_shuff <- dplyr::select(shuffleddata, c(1, 2, 3, 6, 7,10, 28:59, 70:77,))


# Calculate the index for the 90-10 split
index90 = floor(0.9 * nrow(shuffleddata))

train = shuffleddata[1:index90,]
train_naive = data_naive_shuff[1:index90,]
# Create the 10% testing data
test = shuffleddata[(index90 + 1):nrow(shuffleddata),]
test_naive = data_naive_shuff[(index90 + 1):nrow(data_naive_shuff),]

#of the 90 data, 7 fold cross validation

folds = createFolds(train$loglatestPrice, k = 7, list = TRUE)
```

# AIC and BIC

```{r}
MSE.bic = vector()
bic.num = vector()
#MSE.aic = vector()
#aic.num = vector()

for (j in 1:7) {
  #setting respective fold to either training or testing
  train_data = train[-folds[[j]], ] #eliminate columns in testing data
  test_data = train[folds[[j]], ]
  
  #doing best subset selection on our data
  regfit.train = regsubsets(loglatestPrice ~ ., data = train_data,  method= "backward" )
  reg.summary = summary(regfit.train)
  
  #selecting which subset is the best, specifically selecting how many variables it has 
  bic.number = which.min(reg.summary$bic)
  bic.num[j] = bic.number

  #creating the model with the respective number of coefficents
  bic.mdl = coef(regfit.train, bic.number)
  
  # testing the models that are created
  test.mat2 = model.matrix(loglatestPrice ~ ., data = test_data)
  bic.pred = test.mat2[, names(bic.mdl)] %*% bic.mdl
  
  
  # Start with the full model
  full.model = lm(loglatestPrice ~ ., data = train_data)
  
  # Perform backward stepwise selection using AIC
  #stepwise.model = stepAIC(full.model, direction = "backward", trace = FALSE)

  #predict with the 
  #aic.pred <- predict(stepwise.model, newdata = test_data)
  #aic.num[j] <- length(coef(stepwise.model))


  #computing the MSE for each model
  MSE.bic[j] = mean(( test_data$loglatestPrice - bic.pred)**2)
  #MSE.aic[j] = mean((test_data$loglatestPrice - aic.pred)**2)
}

paste("Estimated MSE for BIC is ", sum(MSE.bic))
#paste("Estimated MSE for AIC is ", sum(MSE.aic))
 
bic.num
```

BIC says model with \_\_\_ variables, AIC says model with \_\_\_
variables

# LASSO & Ridge

## LASSO

```{r}
set.seed(1234)

vectorMSElasso = vector()
for (j in 1:7) {
  
  
  train_data = train[-folds[[j]], ]
  test_data = train[folds[[j]], ]
  
  train_data_x = as.matrix(train_data[,-6])
  train_data_y = as.matrix(train_data[, 6])
  test_data_x = as.matrix(test_data[,-6])
  test_data_y = as.matrix(test_data[, 6])
  
  grid = 10^seq(10, -2, length = 100)
  
  lasso.mod.cv = cv.glmnet(train_data_x, train_data_y, alpha = 1, lambda = grid, nfolds = 5)
  
  bestlambda = lasso.mod.cv$lambda.min
  
  lasso.fulldata = glmnet(train_data_x, train_data_y, alpha = 1, lambda = grid)
  lasso.coef = predict(lasso.fulldata , type = "coefficients",s = bestlambda)
  
  lasso.predict = predict(lasso.fulldata, s = bestlambda, newx = test_data_x)
  mse.lasso = mean((test_data_y - lasso.predict)^2)
  vectorMSElasso[j] = mse.lasso
}

mean(vectorMSElasso)
```

## Ridge

```{r}
set.seed(1234)

vectorMSEridge = vector()
for (j in 1:7) {
  
  
  train_data = train[-folds[[j]], ]
  test_data = train[folds[[j]], ]
  
  train_data_x = as.matrix(train_data[,-6])
  train_data_y = as.matrix(train_data[, 6])
  test_data_x = as.matrix(test_data[,-6])
  test_data_y = as.matrix(test_data[, 6])
  
  grid = 10^seq(10, -2, length = 100)
  
  lasso.mod.cv = cv.glmnet(train_data_x, train_data_y, alpha = 0, lambda = grid, nfolds = 5)
  
  bestlambda = lasso.mod.cv$lambda.min
  
  lasso.fulldata = glmnet(train_data_x, train_data_y, alpha = 0, lambda = grid)
  lasso.coef = predict(lasso.fulldata , type = "coefficients",s = bestlambda)
  
  lasso.predict = predict(lasso.fulldata, s = bestlambda, newx = test_data_x)
  mse.lasso = mean((test_data_y - lasso.predict)^2)
  vectorMSEridge[j] = mse.lasso
}
mean(vectorMSEridge)
```

# The Optimal model

```{r}
#lowest mean squared error was Ridge, so we will run this over entire training dataset
train_data_x = as.matrix(train[,-6])
train_data_y = as.matrix(train[, 6])
test_data_x = as.matrix(test[,-6])
test_data_y = as.matrix(test[, 6])
  
grid = 10^seq(10, -2, length = 100)
  
ridge.mod.cv = cv.glmnet(train_data_x, train_data_y, alpha = 0, lambda = grid, nfolds = 5)
plot(ridge.mod.cv, main = "MSE of Ridge with Respective Lambdas")

bestlambda = ridge.mod.cv$lambda.min


lasso.fulldata = glmnet(train_data_x, train_data_y, alpha = 0, lambda = grid)
ridge.coef = predict(lasso.fulldata , type = "coefficients",s = bestlambda)


ridge.predict = predict(lasso.fulldata, s = bestlambda, newx = test_data_x)
mse.ridge = mean((test_data_y - ridge.predict)^2)
paste("mean squared error on test data for ridge is ", mse.ridge)


paste("best lambda for ridge is ", bestlambda)

```

```{r}
# Scale training data and retain scaling parameters
scaling_info = scale(train[, -6])  # Store scaling info for later use
train_data_x = scale(train[, -6], center = attr(scaling_info, "scaled:center"), 
                      scale = attr(scaling_info, "scaled:scale"))
train_data_y = as.numeric(unlist(train[, 6]))

# Scale test data using training scaling parameters
test_data_x = scale(test[, -6], center = attr(scaling_info, "scaled:center"), 
                     scale = attr(scaling_info, "scaled:scale"))
test_data_y = as.numeric(unlist(test[, 6]))

# Define the lambda grid
grid = 10^seq(10, -2, length = 100)

# Combine response and predictors into a data frame for lmridge
train_data = data.frame(train_data_y, train_data_x)
test_data = data.frame(test_data_y, test_data_x)

start.time <- Sys.time()

# Cross-validation to find the best lambda
cv_errors = sapply(grid, function(lambda) {
  model = lmridge(train_data_y ~ ., data = train_data, K = lambda)
  pred = predict(model, newdata = test_data, type = "response")  # Predict on the test set
  mean((test_data_y - pred)^2)  # Calculate mean squared prediction error
})

end.time <- Sys.time()
time.taken <- end.time - start.time


# Identify the best lambda
bestlambda = grid[which.min(cv_errors)]

# Plot MSPE vs. Lambda
plot(log10(grid), cv_errors, type = "b", main = "MSPE of Ridge with Respective Lambdas",
     xlab = "Log10(Lambda)", ylab = "MSPE")

# Fit the ridge regression model with the best lambda
ridge_model = lmridge(train_data_y ~ ., data = train_data, K = bestlambda)

# Extract coefficients
ridge_coef = coef(ridge_model)

# Make predictions on the test set
ridge_predict = predict(ridge_model, newdata = test_data, type = "response")

# Compute MSE on the test set
mse_ridge = mean((test_data_y - ridge_predict)^2)

# Output results
paste("Mean squared error on test data for Ridge is", mse_ridge)
paste("Best lambda for Ridge is", bestlambda)

```

# Comparison with Naive Model

As noted in our report we now consider a model based on what some
relevant field experts consider to be the main features most buyers use
when valuing houses.

```{r}
# Fit the normal linear regression model for naive data 
lm_model_naive = lm(train_naive$loglatestPrice ~ ., data = train_naive)
residuals_naive = resid(lm_model_naive)

naive_predictions <- predict(lm_model_naive, newdata = test_naive)

mse_naive <- mean((test_naive$loglatestPrice - naive_predictions)^2)
paste('The naive model produces MSE',mse_naive,'. Ridge Produces an MSE of ', mse_ridge)
```

## Residual Plots

```{r}
# Residual plot for the naive model
par(mfrow = c(1, 1))  # Set up two plots side by side

# Plot residuals for the optimal model
ggplot(data = data.frame(residuals_naive), aes(x = residuals_naive)) +
  geom_histogram(aes(y = ..density..), bins = 50, fill = "lightblue", color = "black", alpha = 0.7) +
  geom_density(color = "red", size = 1) +
  labs(title = "Residuals for Naive Model", x = "Residuals", y = "Density") +
  theme_minimal()

# Plot residuals for the optimal model
residuals_opt = test_data_y - ridge.predict

ggplot(data = data.frame(residuals_opt), aes(x = residuals_opt)) +
  geom_histogram(aes(y = ..density..), bins = 50, fill = "lightblue", color = "black", alpha = 0.7) +
  geom_density(color = "red", size = 1) +
  labs(title = "Residuals for Optimal Model", x = "Residuals", y = "Density") +
  theme_minimal()
# Reset the plot layout

```

## Summary

```{r}
summary()
#betahat = ridge_coef
#print("fail 1")
#varbetahat = vcov(ridge_model)
#print("fail 2")
#H = hatr(ridge_model)	
#print("fail 3")
#v = nrow(train_data_x) -sum(diag(H))
#print("fail 4")


#sigmahat = (t(train_data_y - train_data_x %*% betahat) %*% (train_data_y - train_data_x%*% betahat)) / v
#print("fail 5")

#varbetahat = sigmahat * varbetahat

#test_stat_func = function(B_i, varbh_i) {
#    return(abs(B_i) / sqrt(varbh_i))
#}

#critical_value <- qt(0.975, df = v)  # 95% confidence level

#for (i in 1:nrow(result_matrix)) {
#  test_stat = test_stat_func(as.numeric(betahat[i]),  varbetahat[i, i])
  
  # Set coefficient to 0 if the test fails (i.e., |test_stat| < critical_value)
#  if (test_stat < critical_value) {
#    result_matrix[i, 2] <- 0  
#  }
#}

# Print the final result matrix
#print(result_matrix)
```
